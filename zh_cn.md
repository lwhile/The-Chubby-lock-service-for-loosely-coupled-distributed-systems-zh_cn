# Chubby: 为松散耦合的分布式系统提供锁服务 

## 摘要 
我们将介绍我们关于Chubby锁服务的一些经验，这是一个为松散耦合式的分布式系统提供的粗粒度锁服务，就像可靠的存储服务一样（尽管容量小）。Chubby提供了一个与分布式文件系统的协同锁很相似的接口，但是设计重点在于可用性和可靠性，而不是高性能。许多个服务实例已经使用了超过一年的时间，这些服务中的大部分都有着几万个客户同时在线的并发量。这篇论文将介绍最初的设计以及预期的用途，并对比实际用途，解释为了适应这些差异，设计做出了哪些修改。

## 1 介绍
这篇论文介绍一个叫做Chubby的锁服务，旨在用于由高速网络连接中等规模的小型机器组成的松散耦合分布式系统。例如，一个Chubby实例（也被称为Chubby单元）可能用于由1Gbit/s的以台网连接起来的一万台四核机器上。大多数Chubby单元会被使用在单数据中心或者机房里，尽管我们至少会部署一个在数千公里外有副本的Chubby单元。

锁服务的目的在于允许多个客户端同步它们的行为，并且在环境的基本信息方面达成一致。对于一个中等规模的客户端集合来说，其主要目标包括可靠性、可用性，以及具有容易理解的语意。吞吐量和存储空间则是次要考虑的目标。Chubby的客户端接口与一个简单的文件系统很相似，这种文件系统会执行整个文件的读写，并且使用协同锁做为加强，同时会对各种类型的事件提供通知机制，例如文件修改。

我们希望Chubby帮助开发者处理系统内的粗粒度同步，特别是能够解决在一组服务器中选出领导者的问题。例如，GFS(谷歌文件系统)使用Chubby锁来指派一个主服务，而Bigtable则在几个方面使用Chubby：1.选举一个主结点，2.允许主结点发现其控制的子结点，允许客户端寻找他们的主结点。另外，GFS和Bigtable都使用Chubby作为一个众所周知并且可用的位置来存储少量的元数据。实际上它们都是将Chubby做为它们分布式数据结构的根。有些服务则使用锁在几个服务之间区分工作（粗粒度）。

在Chubby上线之前，谷歌的许多分布式系统使用```ad hoc```做为主要的选举方式（当工作重复做不会导致不良影响时），或者需要操作人员的介入（需要保证正确性时）。对于前者，Chubby允许了其能保存少量的计算结果。对于后者，Chubby在系统的可用性上实现了重大的提升，不再需要人类在系统出错时手动介入。

如果读者熟悉分布式计算的话，会意识到在几个主要结点间做选举是针对分布式一致性问题的一个例子，并且知道我们需要一个使用异步通信的方案；这个术语描述了绝大多数真实网络的行为，例如以太网和因特网，它们都允许数据包被丢失、延迟、重新排序（从业人员通常需要提防将协议建立在那些对环境做了高度假设的基础之上）。异步一致性问题使用Paxos[12,13]协议作为解决方案。Paxos协议也被用在Oki和Liskov项目上（可以阅读他们关于viewstamped replication的论文[19, §4]）。实际上，到目前为止，我们所有可行的异步一致性协议都使用Paxos作为它们的核心。Paxos并不依赖时间假设来保证安全性，但是需要引入时钟来确保活性；这做到Fischer的研究结果中认为不可能的事[5, §1]。

构建Chubby需要工程上的努力才能完成上面的目标；这其实并不是一项研究工作，我们并不会介绍新的算法或者技术。这篇论文的主要目的是介绍我们做了什么、为什么要这样做，而不是去提倡它。在接下来的章节，我们介绍Chubby的设计和实现，以及它是如何根据经验进行调整的。我们介绍一些使用Chubby的意想不到的方式，以及被证明是错误的特性。我们在文献的其他地方忽略了一些细节，比如一致性算法或者RPC系统。

## 2 设计

### 2.1 合理性
有人可能会认为我们应当将Paxos算法封装成一个库，而不是通过调用某个库去访问一个中心化的锁服务，即便它非常可靠。一个客户端的Paxos库不会依赖于其他的服务（除了名称服务），假设程序员的服务能够以状态机的形式实现，那么也能够为程序员提供一个标准的框架。实际上，我们就提供了这样的一个独立于Chubby的客户端库。

对比客户端库，锁服务有几个优势。第一，我们的开发者在最开始的时候并不会考虑高可用。多数情况下他们的系统都是以从较低负载以及较低的可用性保证开始开发的；代码中并不会有专门的数据结构用来实现一致性协议。随着服务的成熟以及用户的增长，可用性变得越来越重要；副本和初级的选举机制会被添加到已有的设计当中。这种情况下可以使用提供分布式一致性协议的库，加上使用锁服务就可以让维护现有的程序结构以及交互模式变得更加简单。例如，为了选举出一个用来写入已有文件的文件服务的主结点，只需要添加两个语句以及一个RPC参数到现有的系统中即可。获得锁的将成为主结点，通过在写RPC时添加一个额外的整数（对锁获得情况的计数），以及添加一个if语句以拒绝那些计数值低于当前实际值的写请求。比起让现有的服务实现一致性协议，特别是必须得保证兼容性的情况下，我们发现使用这种技术实现起来会更加容易。

第二，我们的许多服务会选择一个主要的服务，或者在他们的组件之间对数据做区分，这些都需要一个能够传播结果的机制。这提醒我们需要允许客户端能够存储数据或者取回少量的数据，即能够读写小文件。这可以通过名称服务器来完成，但我们的经验证明使用锁服务可能会更加适合，一方面是因为能够减少客户端所以来的服务数量，另一方面是因为是共享协议提供的一致性。Chubby在名称服务器上的成功很大一方面是其使用了一致性客户缓存，而不是基于时间的缓存。特别是我们发现开发者非常感激不用去选择缓存的超时时间，例如DNS缓存的生存周期，如果选择不佳的话可能会导致高的DNS负载或者长的客户端故障时间。

第三，对于我们的程序员来说，他们更熟悉基于锁的接口。Paxos的复制状态机以及与排他锁关联的临界区能够为程序员提供循序编程的感觉。许多程序员都遇到过锁，并且认为他们知道如何使用它。讽刺的是，这些程序员通常都是错的，特别是当他们在一个分布式系统中使用锁的时候。很少有人会考虑独立的某台机器发生故障时对异步通信系统中的锁的影响。尽管如此，对于锁的较高熟悉程度，还是能够说服程序员去克服在使用可靠的分布式锁服务中遇到的障碍。

最后，分布式一致性算法使用多数表决的方法来做决策，所以它们使用副本集来达到高可用。例如，Chubby最少需要3个才能保证正常工作，通常使用5个副本。作为对比，如果一个客户端系统使用锁服务，即便是单独的客户端也能获得锁来保证程序的安全性。所以，一个锁服务能够减少客户端所依赖的服务数。还有一点没有提到,可以将锁服务做为通用的选举服务提供给客户端系统，让其正常工作的成员数量少于大多数的时候能够做出正确的决策。在最后一个问题上，有个想象中可能存在的解决方案：使用若干服务作为Paxos协议中的“acceptors“，提供一致性服务。与锁服务相似，一个一致性服务能够让客户端的进程变得更加安全，即便只有一个；一种类似的技术也已经用于减少Byzantine容灾所需要的状态机数量。然而，假设一个一致性服务没有专门用于提供锁，那么这种解决方法将不能用于解决上面提到的问题。

这些论点提出了两个在决策设计中的建议：

- 我们提供了一个锁服务，而不是一个库或者是一致性服务。

- 我们选择服务小文件，允许被选出来的主去传播它们和它们的参数，而不是构造和维护另外一个服务。

下面的决定来自于我们预期的使用方法以及我们的环境：

- 一个通过Chubby传播其主的服务可能拥有几千个客户端。因此，我们必须允许几千个客户端观察该文件，并且最好不要使用太多服务器。

- 客户端和复制服务的副本们可能希望知道服务的主发生了变化。这说明提供一个事件通知机制比起长轮询会更加有用。

- 即便有些客户端不需要长时间轮询文件，还是有其他客户端需要；所以，文件缓存是必要的，这也是为了能够支持更多的开发者。

- 我们的开发者对不直观的缓存语义感到困惑，所以我们更倾向于提供一致性缓存。

- 为了避免经济的损失，我们提供了安全机制，包括访问控制。

有个选择可能会让一些读者感到惊讶，我们并不期望锁被用在只有几秒或者更少的细粒度上面。取而代之的是，我们希望锁是粗粒度的。例如，一个应用可能使用一个锁去选择一个主，这个主在接下来很长一段时间，有可能是几小时甚至是几天，会处理所有的访问请求。这两种使用方式会对锁服务器提出不同的要求。

粗粒度的锁对服务器施加的负载会小很多。特别是，请求锁的频率通常和客户端应用的交互频率只有微弱的关系。粗粒度的锁只能很少的人获取，所以锁服务器的暂时不可用对客户端造成的延迟影响也会减少。从另一个方面来说，在客户端之间传输锁的过程是昂贵的，所以我们不希望任何服务器的故障会导致锁的丢失。因此，对于粗粒度的锁来说在服务器故障中恢复是更加有优势的，这样做的开销也很小，并且在只牺牲部分可用性的情况下，这样的锁只需要中等规模的服务器就能服务数量众多的客户端。

细粒度的锁会得出不同的结论，甚至服务的短时间不可用就可能导致许多客户端产生很高的延迟。性能和服务器的横向扩展能力是非常值得注意的，因为锁服务的事务速率会随着客户端事务速率的增长而增长。这能够带来的好处是能够在锁服务故障时不维护锁，以减少锁的负载，并且频繁得丢失锁对时间的影响也不会太严重，因为锁的有效期很短。（客户端必须做好在网络分区时会丢失锁的准备，所以在锁服务发生故障时也不会导致产生出新的恢复路径。）

Chubby只提供粗粒度的锁。幸运的是，客户端也可以直接根据他们的应用实现细粒度锁。一个应用可以将它们的锁区分成组，并且使用Chubby的粗粒度锁将这些组分配到指定应用的服务上。维护这些细粒度的锁需要一点状态信息；服务器只需要保持一个非易失，单调递增并且很少更新的计数器。客户端可以在解锁时了解丢失的锁，如果使用了定长的租约信息，协议可以变得简单高效。使用这种方案的好处就是我们的开发者只需要提供能够支持它们的负载的服务器配置即可，而不用关心如何实现复杂的一致性。

### 2.2 系统结构

Chubby有两个通过RPC通信的重要组件，一个是服务端，以及一个能够让客户端程序互相连接的库；参见图1。所有的Chubby客户端和服务器的通讯都在客户端库处理。第三个代理服务器组件是可选的，一个代理服务，这个将在3.1节讨论。

![图1：系统结构](http://okxn16mtf.bkt.clouddn.com/2017-12-09%2019:41:57%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png)

一个Chubby单元包含一小组服务器（通常是5个）来做副本集，以便于减少失败的可能性（例如，在不同的机架上的服务器）。副本集采用分布式一致性协议来选择主节点；这个主节点必须是多数副本集选举出来的，并且保证那些副本集在主节点的任期内不能选出另外的主节点。这个主节点的租约定期通过副本集更新，以便让主继续赢得大多数选票。

副本会维护一个简单数据库的副本，但只有主会初始化该数据库的读写操作。所有其他副本都是简单从主那里复制更新，并使用一致性协议进行发送。

客户端通过发送主的定位请求到在DNS里的副本列表来寻找主。非主的副本会响应该请求主的位置。一旦一个客户端定位到主了，该客户端会将所有请求直接发送到主上，直到其无法响应客户端的请求或者告诉客户端它不是主了。写请求会通过一致性算法传播到所有副本上；这样的请求会在写入大多数副本后会认为已经被接受。读请求会通过主满足安全性；如果主的租约没有过期，那么就是安全的，因为没有其他主存在的可能。如果一个主宕机了，当其租约过期时，其他的副本会运行选举协议选出新的主。一个新的主一般会在几秒内被选出来。举个例子，两个最近的选举分别用了6s和4s，但是我们会看到数值其实高达30s(§4.1)。


如果副本故障并且没有在几小时之内恢复，一个简单的替换系统会从一个空闲的池中选择一个新的机器，然后在上面启动锁服务的二进制。接下来再更新DNS表，使用新机器的IP替换掉故障机器的IP。当前的主会定时轮询服务器，最终关注到变化后，会更新数据库中中Chubby单元的成员列表。这个列表会使用常规的复制协议在所有成员之间保持一致。与此同时，新的副本会从存储在文件系统的数据库备份中取得最近几次的拷贝，并且在活跃的的那些副本那里取得更新。一旦新的那个副本已经处理了一个请求，并且当前的主正在等待提交，那么这个副本就允许在选举中投票以选出新的主。

2.3 文件，目录，和句柄

Chubby对外导入一个与Unix很像但比Unix简单的文件系统接口。在通常情况下，它包含了一个严格的文件和目录树，并用斜杠分隔名字组件。一个典型的名字是：

> /ls/foo/wombat/pouch

```ls```是所有Chubby的文件名字通用的前缀，并且代表了锁服务。第二个组件（foo）是Chubby单元的名字；它通过DNS查找解析到一个或多个Chubby服务器。一个特殊的单元名```local```表明客户端本地的Chubby单元应该被使用。该单元通常是在同一个构建环境中，因此是最有可能被访问到的一个。名字的剩余部分，```/wombat/pounch```,在被命名的Chubby单元中进行解析。同样跟随Unix，每一个目录都包含一个文件列表和目录列表，当中的每个文件都包含了一串未被解释的字节。

由于Chubby的命名结构酷似一个文件系统，我们可以通过自己的专用API和其他文件系统使用的接口使应用程序可用，例如GFS。这能够显著减少写基础的浏览或维护命名空间工具的需要，同时也降低了Chubby用户的学习成本。