# Chubby: 为松散耦合的分布式系统提供锁服务 

## 摘要 
我们将介绍我们关于Chubby锁服务的一些经验，这是一个为松散耦合式的分布式系统提供的粗粒度锁服务，就像可靠的存储服务一样（尽管容量小）。Chubby提供了一个与分布式文件系统的协同锁很相似的接口，但是设计重点在于可用性和可靠性，而不是高性能。许多个服务实例已经使用了超过一年的时间，这些服务中的大部分都有着几万个客户同时在线的并发量。这篇论文将介绍最初的设计以及预期的用途，并对比实际用途，解释为了适应这些差异，设计做出了哪些修改。

## 1 介绍
这篇论文介绍一个叫做Chubby的锁服务，旨在用于由高速网络连接中等规模的小型机器组成的松散耦合分布式系统。例如，一个Chubby实例（也被称为Chubby单元）可能用于由1Gbit/s的以台网连接起来的一万台四核机器上。大多数Chubby单元会被使用在单数据中心或者机房里，尽管我们至少会部署一个在数千公里外有副本的Chubby单元。

锁服务的目的在于允许多个客户端同步它们的行为，并且在环境的基本信息方面达成一致。对于一个中等规模的客户端集合来说，其主要目标包括可靠性、可用性，以及具有容易理解的语意。吞吐量和存储空间则是次要考虑的目标。Chubby的客户端接口与一个简单的文件系统很相似，这种文件系统会执行整个文件的读写，并且使用协同锁做为加强，同时会对各种类型的事件提供通知机制，例如文件修改。

我们希望Chubby帮助开发者处理系统内的粗粒度同步，特别是能够解决在一组服务器中选出领导者的问题。例如，GFS(谷歌文件系统)使用Chubby锁来指派一个主服务，而Bigtable则在几个方面使用Chubby：1.选举一个主结点，2.允许主结点发现其控制的子结点，允许客户端寻找他们的主结点。另外，GFS和Bigtable都使用Chubby作为一个众所周知并且可用的位置来存储少量的元数据。实际上它们都是将Chubby做为它们分布式数据结构的根。有些服务则使用锁在几个服务之间区分工作（粗粒度）。

在Chubby上线之前，谷歌的许多分布式系统使用```ad hoc```做为主要的选举方式（当工作重复做不会导致不良影响时），或者需要操作人员的介入（需要保证正确性时）。对于前者，Chubby允许了其能保存少量的计算结果。对于后者，Chubby在系统的可用性上实现了重大的提升，不再需要人类在系统出错时手动介入。

如果读者熟悉分布式计算的话，会意识到在几个主要结点间做选举是针对分布式一致性问题的一个例子，并且知道我们需要一个使用异步通信的方案；这个术语描述了绝大多数真实网络的行为，例如以太网和因特网，它们都允许数据包被丢失、延迟、重新排序（从业人员通常需要提防将协议建立在那些对环境做了高度假设的基础之上）。异步一致性问题使用Paxos[12,13]协议作为解决方案。Paxos协议也被用在Oki和Liskov项目上（可以阅读他们关于viewstamped replication的论文[19, §4]）。实际上，到目前为止，我们所有可行的异步一致性协议都使用Paxos作为它们的核心。Paxos并不依赖时间假设来保证安全性，但是需要引入时钟来确保活性；这做到Fischer的研究结果中认为不可能的事[5, §1]。

构建Chubby需要工程上的努力才能完成上面的目标；这其实并不是一项研究工作，我们并不会介绍新的算法或者技术。这篇论文的主要目的是介绍我们做了什么、为什么要这样做，而不是去提倡它。在接下来的章节，我们介绍Chubby的设计和实现，以及它是如何根据经验进行调整的。我们介绍一些使用Chubby的意想不到的方式，以及被证明是错误的特性。我们在文献的其他地方忽略了一些细节，比如一致性算法或者RPC系统。

## 2 设计

### 2.1 合理性
有人可能会认为我们应当将Paxos算法封装成一个库，而不是通过调用某个库去访问一个中心化的锁服务，即便它非常可靠。一个客户端的Paxos库不会依赖于其他的服务（除了名称服务），假设程序员的服务能够以状态机的形式实现，那么也能够为程序员提供一个标准的框架。实际上，我们就提供了这样的一个独立于Chubby的客户端库。

对比客户端库，锁服务有几个优势。第一，我们的开发者在最开始的时候并不会考虑高可用。多数情况下他们的系统都是以从较低负载以及较低的可用性保证开始开发的；代码中并不会有专门的数据结构用来实现一致性协议。随着服务的成熟以及用户的增长，可用性变得越来越重要；副本和初级的选举机制会被添加到已有的设计当中。这种情况下可以使用提供分布式一致性协议的库，加上使用锁服务就可以让维护现有的程序结构以及交互模式变得更加简单。例如，为了选举出一个用来写入已有文件的文件服务的主结点，只需要添加两个语句以及一个RPC参数到现有的系统中即可。获得锁的将成为主结点，通过在写RPC时添加一个额外的整数（对锁获得情况的计数），以及添加一个if语句以拒绝那些计数值低于当前实际值的写请求。比起让现有的服务实现一致性协议，特别是必须得保证兼容性的情况下，我们发现使用这种技术实现起来会更加容易。

第二，我们的许多服务会选择一个主要的服务，或者在他们的组件之间对数据做区分，这些都需要一个能够传播结果的机制。这提醒我们需要允许客户端能够存储数据或者取回少量的数据，即能够读写小文件。这可以通过名称服务器来完成，但我们的经验证明使用锁服务可能会更加适合，一方面是因为能够减少客户端所以来的服务数量，另一方面是因为是共享协议提供的一致性。Chubby在名称服务器上的成功很大一方面是其使用了一致性客户缓存，而不是基于时间的缓存。特别是我们发现开发者非常感激不用去选择缓存的超时时间，例如DNS缓存的生存周期，如果选择不佳的话可能会导致高的DNS负载或者长的客户端故障时间。

第三，对于我们的程序员来说，他们更熟悉基于锁的接口。Paxos的复制状态机以及与排他锁关联的临界区能够为程序员提供循序编程的感觉。许多程序员都遇到过锁，并且认为他们知道如何使用它。讽刺的是，这些程序员通常都是错的，特别是当他们在一个分布式系统中使用锁的时候。很少有人会考虑独立的某台机器发生故障时对异步通信系统中的锁的影响。尽管如此，对于锁的较高熟悉程度，还是能够说服程序员去克服在使用可靠的分布式锁服务中遇到的障碍。


<<<<<<< HEAD
**经过润色，可能不太好**
****
###2.1 基本原理
有一种观点认为我们应该建立一个基于Paxos算法的library而不是建立一个基于访问集中锁服务的library。即使后者高度可靠。假设程序员的服务能作为状态机实现，一个Paxos library的客户端would depend on no other servers (besides
the name service)【这个翻译不出来，不知道怎么表达。可不可以翻译成：不会依赖其他的服务器。那个括号里面的真不知道什么意思】，并且会提供一个标准的框架给程序员。确实，我们提供了一个是独立于Chubby之外的client 库

尽管如此，在客户端库锁服务有些优势。首先，我们开发者有时候不打算做与高效性相悖的事情。通常他们的系统以一个little load和loose availability guarantees的原型开始。基于一致性协议使用起来代码都不约而同的没有良好的结构。随着服务日趋成熟并且获取更多客户（gains clients），有效、可用性变得更加重要。复制和初选然后添加到现有的设计（replication and primary election are then added to an existing design）。当这些被封装成库提供了分布式共识（distributed consensus）的时候，一个锁服务器让它变得更简单去维护现存的程序架构和通信模式。举个例子，
to elect a master which then writes to an existing file server requires adding just two statements and one RPC parameter to an existing system: One would acquire a lock to become master, pass an additional integer (the lock acquisition count) with the write RPC, and add an if-statement to the file server to reject the write if the acquisition count is lower than the current value (to guard against delayed packets).我们发现这技巧比making现有的服务器在一致性协议下participate要简单，并且特别是在transition period（灰度？）时保持兼容性的时候。
=======
最后，分布式一致性算法使用多数表决的方法来做决策，所以它们使用副本集来达到高可用。例如，Chubby最少需要3个才能保证正常工作，通常使用5个副本。作为对比，如果一个客户端系统使用锁服务，即便是单独的客户端也能获得锁来保证程序的安全性。所以，一个锁服务能够减少客户端所依赖的服务数。还有一点没有提到,可以将锁服务做为通用的选举服务提供给客户端系统，让其正常工作的成员数量少于大多数的时候能够做出正确的决策。在最后一个问题上，有个想象中可能存在的解决方案：使用若干服务作为Paxos协议中的“acceptors“，提供一致性服务。与锁服务相似，一个一致性服务能够让客户端的进程变得更加安全，即便只有一个；一种类似的技术也已经用于减少Byzantine容灾所需要的状态机数量。然而，假设一个一致性服务没有专门用于提供锁，那么这种解决方法将不能用于解决上面提到的问题。

这些论点提出了两个在决策设计中的建议：

- 我们提供了一个锁服务，而不是一个库或者是一致性服务。

- 我们选择服务小文件，允许被选出来的主去传播它们和它们的参数，而不是构造和维护另外一个服务。

下面的决定来自于我们预期的使用方法以及我们的环境：

- 一个通过Chubby传播其主的服务可能拥有几千个客户端。因此，我们必须允许几千个客户端观察该文件，并且最好不要使用太多服务器。

- 客户端和复制服务的副本们可能希望知道服务的主发生了变化。这说明提供一个事件通知机制比起长轮询会更加有用。

- 即便有些客户端不需要长时间轮询文件，还是有其他客户端需要；所以，文件缓存是必要的，这也是为了能够支持更多的开发者。

- 我们的开发者对不直观的缓存语义感到困惑，所以我们更倾向于提供一致性缓存。

- 为了避免经济的损失，我们提供了安全机制，包括访问控制。

有个选择可能会让一些读者感到惊讶，我们并不期望锁被用在只有几秒或者更少的细粒度上面。取而代之的是，我们希望锁是粗粒度的。例如，一个应用可能使用一个锁去选择一个主，这个主在接下来很长一段时间，有可能是几小时甚至是几天，会处理所有的访问请求。这两种使用方式会对锁服务器提出不同的要求。

粗粒度的锁对服务器施加的负载会小很多。特别是，请求锁的频率通常和客户端应用的交互频率只有微弱的关系。粗粒度的锁只能很少的人获取，所以锁服务器的暂时不可用对客户端造成的延迟影响也会减少。从另一个方面来说，在客户端之间传输锁的过程是昂贵的，所以我们不希望任何服务器的故障会导致锁的丢失。因此，对于粗粒度的锁来说在服务器故障中恢复是更加有优势的，这样做的开销也很小，并且在只牺牲部分可用性的情况下，这样的锁只需要中等规模的服务器就能服务数量众多的客户端。

细粒度的锁会得出不同的结论，甚至服务的短时间不可用就可能导致许多客户端产生很高的延迟。性能和服务器的横向扩展能力是非常值得注意的，因为锁服务的事务速率会随着客户端事务速率的增长而增长。这能够带来的好处是能够在锁服务故障时不维护锁，以减少锁的负载，并且频繁得丢失锁对时间的影响也不会太严重，因为锁的有效期很短。（客户端必须做好在网络分区时会丢失锁的准备，所以在锁服务发生故障时也不会导致产生出新的恢复路径。）

Chubby只提供粗粒度的锁。幸运的是，客户端也可以直接根据他们的应用实现细粒度锁。一个应用可以将它们的锁区分成组，并且使用Chubby的粗粒度锁将这些组分配到指定应用的服务上。维护这些细粒度的锁需要一点状态信息；服务器只需要保持一个非易失，单调递增并且很少更新的计数器。客户端可以在解锁时了解丢失的锁，如果使用了定长的租约信息，协议可以变得简单高效。使用这种方案的好处就是我们的开发者只需要提供能够支持它们的负载的服务器配置即可，而不用关心如何实现复杂的一致性。
>>>>>>> upstream/master
